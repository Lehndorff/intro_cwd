---
title: "Project 3"
author: "Hendrik Orem, Ph.D., with thanks to Jameson Watts"
date: "03/16/2023"
output:
  pdf_document:
    df_print: kable
    fig_width: 11
    fig_height: 8
  html_document:
    df_print: paged
---





## Setup
```{r setup, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(tidytext)
library(caret)
library(fastDummies)
library(randomForest)
# https://www.openml.org/d/1590

tourney_data <- read_csv("tourney_data.csv") %>% 
  mutate(rank=log2(rank)) %>% # converts 1,2,4,8,16, etc to 0,1,2,3,4
  select(-Season,-class,-`Coach(es)`,-`NCAA Tournament`) %>% #either not helpful, or duplicative
  filter(!is.na(Pts_for)) %>% #removes 1 case
  mutate(across("AP Pre":"AP Final",function(x){ifelse(is.na(x),"Not 25",paste("Top",ceiling(x/5)*5))})) %>% #data cleaning
  mutate(across(Conf_W:Conf_Pct,function(x){ifelse(is.na(x),0,x)})) #%>% #fill missing info

skimr::skim(tourney_data)

tourney_data2<-tourney_data %>%
  fastDummies::dummy_cols(remove_selected_columns = T) %>% #simple dummy cols
  arrange(tourn_year) %>% mutate(lag_rank=lag(rank)) %>% mutate(lag_rank=ifelse(is.na(lag_rank),6,lag_rank)) %>% #how did the team do last year
  mutate(rank=as.numeric(rank<=4))# <=4 = Did team make the sweet 16 (one of the final 2^4 teams)
table(tourney_data2$rank)
```

## Some questions about the data

Please try at least a few of the following:

* Run PCA on the dataset. How many principal components do you need to explain half the variation? 
* Can you give some interpretation of the principal components?
* Look at the scree plot. How many PCs would you choose based on that?
* Are the first few Principal Components good predictors of income_above_50k?
* How well can you predict income_above_50k using the first 5 or 6 principal components? How about only the first 2?
* Can you gain any insights into the data based on k-means clustering? 
* Can you visualize and interpret some or all of your clusters?
* Using any and all techniques we have learned, build the best predictive model for income_above_50k that you can. What are your best features? What model did you use? What interpretations can you draw?
* What metric can you use to assess model performance? Why is that a good choice of metric in this case?
* What are some key insights you found through your analysis?

Please remember: a statement like "PC2 is not a meaningful predictor for our modeling problem" is a great insight; sometimes things don't work!

10 pts PCA

* analysis and interpretation of factor loadings
* discussion of scree plot and/or analysis of some density plots of PCs
- the screeplot suggests that the proper number of principle components is 3. 
* meaningful interpretation / discussion of conclusions 
- PC1 is correlated with strength of record and inversely with seed (corelated with better seeds) mean that PC1 represents the best seeded teams who also performed well throughout the year. 
- PC2 is inversely correlated with conference win percentage (as well as conference wins and losses). This indicates teams that struggled in conference. 
- PC3 is inversely correlated with both points for and points against suggest that these are teams with strong defenses but poor offenses. 

```{r}
pr_tourney <- prcomp(x = select(tourney_data2,-rank), scale = T, center = T)
summary(pr_tourney)

screeplot(pr_tourney, type = "lines")

rownames_to_column(as.data.frame(pr_tourney$rotation)) %>% 
  select(1:4) %>% 
  filter(abs(PC1) >= 0.25 | abs(PC2) >= 0.25 | abs(PC3) >= 0.25) %>% 
  View()

prc <- bind_cols(select(tourney_data2,rank),as.data.frame(pr_tourney$x)) %>% 
  select(1:4) %>% 
  rename("strong_record_seed" = PC1) %>% 
  rename("struggle_in_conf" = PC2) %>% 
  rename("defense_focus" = PC3) 
head(prc)

prc %>% 
  pivot_longer(cols = -rank,names_to = "component",values_to = "loading") %>% 
  ggplot(aes(x=loading, fill=factor(rank),group=rank))+
  geom_density(alpha=0.5)+
  facet_wrap(.~component,scales = "free_y")


```

10 pts k-means

* discussion for choosing number of clusters
  - We conducted unsupervised learning for various cluster sizes. We looked at two to 10 clusters and charted those by strength of record (SRS) and overall win percentage (Ovr_Pct). We also examined three to five clusters by rank in some density plots. The clusters for our data do not tend to results in visually distinct groupings.
* analysis of cluster centers
  - The clusters do somewhat distinguish between teams with strong records and high win percentages and teams with less strong records and low win percentages. We do have an issue with overlapping clusters. Our models did not find the clusters to be of importance with predicting rank.
* bivariate chart(s) against meaningful variables and/or analysis of density plots
x
* meaningful interpretation / discussion of conclusions 
We chose not to include clusters in our final model as none of the clusters were identified as a variable of importance.

```{r}
# names(tourney_data2)

tourney_cl = select(tourney_data2,-rank)

kclust3 <- kmeans(tourney_cl, centers = 3)

tourney_clustered3 = augment(kclust3, tourney_data2)

# tourney_clustered3 %>% select(rank, .cluster)

tourney_clustered3 %>% pivot_longer(c(rank),names_to = "feature") %>%
  ggplot(aes(value, fill=.cluster))+
  geom_density(alpha=0.3)+
  facet_wrap(~feature)

kclust4 <- kmeans(tourney_cl, centers = 4)

tourney_clustered4 = augment(kclust4, tourney_data2)

# tourney_clustered4 %>% select(rank, .cluster)

tourney_clustered4 %>% pivot_longer(c(rank),names_to = "feature") %>%
  ggplot(aes(value, fill=.cluster))+
  geom_density(alpha=0.3)+
  facet_wrap(~feature)

kclust5 <- kmeans(tourney_cl, centers = 5)

tourney_clustered5 = augment(kclust5, tourney_data2)

# tourney_clustered5 %>% select(rank, .cluster)

tourney_clustered5 %>% pivot_longer(c(rank),names_to = "feature") %>%
  ggplot(aes(value, fill=.cluster))+
  geom_density(alpha=0.3)+
  facet_wrap(~feature)

#four clusters has the most distinct distributions; cluster 2 are the biggest losers,  clusters 1 & 4 mostly losing, and cluster 3 winners
tourney_clustered4 %>% group_by(rank, .cluster) %>% summarize(n=n())

kclusts <- tibble(k = 1:9) %>%
mutate(
kclust = map(k, ~kmeans(tourney_cl, .x)),
glanced = map(kclust, glance),
augmented = map(kclust, augment, tourney_cl)
)

clusterings <- kclusts %>%
unnest(glanced, .drop = TRUE)
ggplot(clusterings, aes(k, tot.withinss)) +
geom_line() 

assignments <- kclusts %>%
unnest(augmented)

ggplot(assignments, aes(Ovr_Pct, SRS)) +
geom_point(aes(color = .cluster), alpha=0.3) +
facet_wrap(~ k)


#hierarchical clustering

hclustr <- hclust(d=dist(tourney_cl))
summary(hclustr)

plot(hclustr)
abline(h=3, col="red")

cluster <- cutree(hclustr, k=3)
tourney_hcl <- tourney_data2 %>%
add_column(cluster) %>%
mutate(cluster=as_factor(cluster))

tourney_hcl %>% group_by(rank, cluster) %>% summarize(n=n())

tourney_hcl %>%
pivot_longer(c(rank),names_to = "feature") %>%
ggplot(aes(value, fill=cluster))+
geom_density(alpha=0.3)+
facet_wrap(~feature)


```


10 pts supervised learning

* feature engineering / selection, whether with PCA or otherwise
- for simplicity we selected 20 predictors that were the most important in an early version of a random forest model. This happens to include the two of the PCA variables. 
* interpretation of variable importance, coefficients if applicable
- With this subset of variables, number of wins (Ovr_W), strength of record (SRS) and strong_record_seed (the principle component for teams with the best seeds AND best records) are the most important variable. Strength of schedule (SOS) was also important.  
* justification of choice of metric
- We evaluated our models in two ways. First, using kappa and ROC AUC for the out of sample for test data prior to 2023. This allowed us to test the predictive power of the model while not being overly reliant on accuracy -- most teams do not make the Sweet 16. To make our model result more applicable, we also tested our model against the 2023 tournament results. We required that the model only predict 16 teams for this test, so our evaluation metric for this test was number of Sweet 16 correctly predicted. 
* discussion of choice or tuning of hyperparameters, if any
- GBM models requires interaction.depth, n.trees, shrinkage and n.minobsinnode. We tuned based on a grid of these values.
* meaningful discussion of predictive power and conclusions from model
- Our model successfully predicted 11 of the 16 in the 2023 tournament. Our model failed to predicted Cinderella Princeton as well as Michigan State, Florida Atlantic Owls, Arkansas Razorbacks, Tennessee Volunteers. I their place, the model predicted high seeded teams - Duke, Arizona, Purdue, Kansas, Marquette. 



```{r}
table(prc$rank==tourney_clustered5$rank) #Check that prc and tourney_clustered4 are ordered the same

full_tourney_features<-tourney_clustered5 %>% 
  cbind(prc %>% select(-rank)) %>% 
  select("rank","SRS","strong_record_seed","Ovr_W","SOS","Seed","tourn_year",
       "struggle_in_conf","Ovr_Pct","defense_focus","Pts_against","Pts_for","Ovr_L",
       "Conf_Pct","Conf_W","AP Final_Not 25","lag_rank","Conf_L",
       "team_Syracuse Orange Men's Basketball","AP High_Top 5","AP Pre_Not 25"
       )

test_2023<-full_tourney_features %>% 
  filter(tourn_year==2023)

full_tourney_features<-full_tourney_features %>% 
  filter(tourn_year!=2023)


set.seed(504)
train<-full_tourney_features %>% 
  group_by(rank) %>% #Sample within outcomes to ensure representativeness
  sample_frac(.8)
test<-full_tourney_features %>% 
  anti_join(train)

control <- trainControl(method = "boot", number = 3)
# control <- trainControl(method = "cv", number = 5)
# fit_rf <- train(rank ~ .,
#              data = train,
#              method = "rf",
#              tuneLength = 10,
#              trControl = control)
# 
# plot(varImp(fit_rf),top=20)
# 
# pred <- predict(fit_rf, newdata=test)
# 
# best_thresh<-function(x){
#   pred2<-as.numeric(pred>x)
#   output<-as.numeric(confusionMatrix(factor(pred2),factor(test$rank))$overall[2])
#   return(output)
# }
# 
# cutoff<-data.frame(thresh=seq(0,1,.01)) %>%
#   mutate(best=sapply(thresh,best_thresh)) %>%
#   filter(best==max(best)) %>%
#   pull(thresh)
# pred2<-as.numeric(pred>cutoff[1])
# 
# confusionMatrix(factor(pred2),factor(test$rank))
# 
# pred <- predict(fit_rf, newdata=test_2023)
# 
# 
# nth_val<-sort(pred,decreasing = T)[2^4] #For current year, should only predicted as many teams as is actually possible
# pred2<-as.numeric(pred>=nth_val)
# table(pred2)
# 
# confusionMatrix(factor(pred2),factor(test_2023$rank))


#model GM
Grid <- expand.grid(interaction.depth=c(1, 5, 10, 20), n.trees = (1:10)*50,
                   shrinkage=c(0.01, 0.001)
                   ,
                   n.minobsinnode=1:10
                   )


fit_gbm <- train(rank ~ .,
             data = train, 
             method = "gbm",
             verbose=FALSE,
             tuneGrid=Grid,
             trControl = control)
library(gbm)
plot(varImp(fit_gbm))

pred <- predict(fit_gbm, newdata=test)

best_thresh<-function(x){
  pred2<-as.numeric(pred>x)
  output<-as.numeric(confusionMatrix(factor(pred2),factor(test$rank))$overall[2])
  return(output)
}

cutoff<-data.frame(thresh=seq(0,1,.01)) %>%
  mutate(best=sapply(thresh,best_thresh)) %>%
  filter(best==max(best)) %>%
  pull(thresh)
pred2<-as.numeric(pred>cutoff[1])

confusionMatrix(factor(pred2),factor(test$rank))

library(pROC)

myRoc <- roc(test$rank, pred)
plot(myRoc)
auc(myRoc)

pred <- predict(fit_gbm, newdata=test_2023)

nth_val<-sort(pred,decreasing = T)[2^4] #For current year, should only predicted as many teams as is actually possible
pred_gbm<-as.numeric(pred>=nth_val)
table(pred_gbm)

confusionMatrix(factor(pred_gbm),factor(test_2023$rank))

# #knn
# 
# fit_knn <- train(rank ~ .,
#              data = train, 
#              method = "knn",
#              tuneLength = 15,
#              trControl = control)
# 
# pred <- predict(fit_knn, newdata=test_2023)
# 
# nth_val<-sort(pred,decreasing = T)[2^4] #For current year, should only predicted as many teams as is actually possible
# pred_knn<-as.numeric(pred>=nth_val)
# table(pred_knn)
# 
# confusionMatrix(factor(pred_knn),factor(test_2023$rank))
# 
# ##naive bayes, NOT WORKING
# # fit_nv <- train(rank ~ .,
# #              data = train, 
# #              method = "naive_bayes",
# #              tuneGrid = expand.grid(usekernel = c(T,F), laplace = T, adjust = T),
# #              metric = "Kappa",
# #              trControl = trainControl(method = "cv"))
# 
# #glm, NOT GREAT
# fit_glm <- train(as.factor(rank) ~ .,
#              data = train, 
#              trControl = control,
#              method = "glm",
#              family = "binomial")
# 
# 
# pred_glm <- predict(fit_glm, newdata=test_2023)
# 
# confusionMatrix(factor(pred_glm),factor(test_2023$rank))
# 
# #xb tree
# fit_xb <- train(rank ~ .,
#              data = train, 
#              method = "xgbTree",
#              verbose=FALSE,
#              trControl = control)
# 
# 
# pred <- predict(fit_xb, newdata=test_2023)
# 
# 
# nth_val<-sort(pred,decreasing = T)[2^4] #For current year, should only predicted as many teams as is actually possible
# pred_xb<-as.numeric(pred>=nth_val)
# 
# confusionMatrix(factor(pred_xb),factor(test_2023$rank))

#select tree as final model!

prediction_output<-tourney_data %>% 
  filter(tourn_year==2023) %>% 
  mutate(rank=as.numeric(rank<=4)) %>% # <=4 = Did team make the sweet 16 (one of the final 2^4 teams)
  mutate(prediction=pred_gbm,raw_pred=pred) %>% 
  select(team, Seed, rank, prediction,raw_pred) %>% 
  mutate(correct=rank==prediction)

```


Please be prepared to 

* Submit your Rmd + compiled html or pdf, *and*
* Present your findings to the class in a compelling way, speaking for 10 minutes or so. You don't need to cover everything in your analysis that you submit to me, focus on the fun / interesting / compelling highlights or challenges.
